{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Support Vector Machines\n",
    "\n",
    "---\n",
    "## Support Vector Machine\n",
    "\n",
    "\n",
    "We're going to add another model to our toolkit: Support Vector Machine or SVM. \n",
    "\n",
    "The SVM was the original king of machine learning as it used an insightful trick to be able to quickly make predictions after a model was fit. While new models have arisen since it's heyday 10 years ago, it's still a powerful and effective model. Let's learn a little bit about it.\n",
    "\n",
    "### Key Terms:\n",
    "- Support Vector Machine\n",
    "- Baseline Baseline Accuracy\n",
    "- Model Hyperparameter\n",
    "\n",
    "### Expected Knowledge:\n",
    "- Manipulating data with Pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:43.660152Z",
     "start_time": "2020-07-30T06:30:40.433337Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, auc, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import missingno as msgn\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use sklearn's ```make_blobs``` to generate some simple data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:45.083972Z",
     "start_time": "2020-07-30T06:30:44.896046Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above a line is 'hyperplane' in two dimensions. So our classifier needs to draw a straight line separating the two blobs. We could do this by hand since we can easily see how to draw a line separating the two classes. \n",
    "\n",
    "What is the problem with this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem** there is more than one possible line that can perfectly discriminate between the two classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:46.410405Z",
     "start_time": "2020-07-30T06:30:46.228846Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "plt.plot([0.4], [2.1], 'x', color='green', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.5), (0.7, 1.2), (-0.1, 2.8)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different lines each can distinguish perfectly between the two samples. However, which line you choose will impact how new data points are classified.  Consider the red  \"X\" in this plot: it will be assigned a different label depending on which line we use as our classifier. So it's not enough to simply draw a hyperplane through our feature space. We need to consider other criteria.\n",
    "\n",
    "### How Support Vector Machines Work\n",
    "Support Vector Machines don't simply generate a hyperplane or in the case above a zero-width line. Instead they draw around each line a margin of some width, up to the nearest point in each of the blobs. It might look as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:47.245922Z",
     "start_time": "2020-07-30T06:30:47.052481Z"
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "\n",
    "for m, b, d in [(1, 0.6, 0.33), (0.7, 1.2, 0.55), (-0.1, 2.8, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM the line that maximizes the margin gives us the optimal model. This is an example of a *maximum margin estimator*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:47.924620Z",
     "start_time": "2020-07-30T06:30:47.915248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "svm = SVC(kernel='linear', C=1E10)\n",
    "\n",
    "\n",
    "# Fit\n",
    "svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function below to plot the decision boundary of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:48.483875Z",
     "start_time": "2020-07-30T06:30:48.475056Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:48.990231Z",
     "start_time": "2020-07-30T06:30:48.783826Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "plot_svc_decision_function(svm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line (aka hyperplane denoted by the solid line) is the line that maximizes the margin between the two sets of points. The margin in this case is the distance from the line to each of the dotted lines. Notice that a few of the training points just touch the margin. These points are the critical elements of this model, and are known as the support vectors, hence the name. We can access these points in ```sklearn``` using the attribute of our fitted model ```svm.support_vectors_ attribute```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:49.370638Z",
     "start_time": "2020-07-30T06:30:49.363415Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that only the position of the support vectors matter and any points further from the margin and on the correct side do not modify the solution. Practically speaking these points don't contribute to the loss function used to fit the model and their position and number don't matter provided they are on the correct side of the margin.\n",
    "\n",
    "The plots below illustrate this, for example, if we plot the model learned from the first 60 points and then first 180 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:50.952051Z",
     "start_time": "2020-07-30T06:30:50.676552Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 180]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left we have the support vectors for 60 training points. On the right, we have triple the number of training points, but the model has not changed: the three support vectors computed from 60 points are the same as computed from 180 data points. SVMs are insensitive to the exact behavior outliers and is one of the strengths of the SVM approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Kernel \n",
    "\n",
    "The data above was easily separable. What happens when we don't have easily separated data? How does SVM work?\n",
    "\n",
    "The idea is mapping the non-linear separable data-set into a higher dimensional space where we can find a hyperplane that can separate the samples. Consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:51.720135Z",
     "start_time": "2020-07-30T06:30:51.539580Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly SVM is not providing a credible solution to classifying this data set. However, consider if we project this data into 3-dimensions as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:52.168331Z",
     "start_time": "2020-07-30T06:30:52.164261Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function will define our projection. In this case we're using a radial basis function.\n",
    "z = np.exp(-((X) ** 2).sum(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:30:52.743482Z",
     "start_time": "2020-07-30T06:30:52.478704Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], z, c=y, s=50, cmap='viridis')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "\n",
    "plot_3D()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting into a higher dimension the data becomes trivially linearly separable. We could draw hyperplane at ```z=0.6``` and achieve a perfect separation.\n",
    "\n",
    "This isn't arbitrary since we chose our projection carefully. We centered our radial function in the right location (i.e. centered on the yellow blob). If we hadn't we would not have had an easily separable result. In practice, the need to make such a choice is problematic and it would be nice to somehow automatically find the best basis functions to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach is to compute a basis function centered at every point in the data, and let the SVM algorithm sift through the results. This approach is based on a similarity relationship (or kernel) between each pair of points and is known as a kernel transformation and involve projecting N points into N dimensions.\n",
    "\n",
    "### The Kernel Trick\n",
    "This type of projection (N points into N dimensions) is computationally intensive as N becomes large. However, because of a procedure known as the kernel trick, we can fit to the kernel-transformed data can be done implicitly. Meaning we don't need to build the full N-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Kernel in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit-Learn, we can apply the kernel trick simply by changing the linear kernel to an RBF (radial basis function) kernel, when we instantiate the SVM classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:17.547676Z",
     "start_time": "2020-07-30T06:31:17.528339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "svmk = SVC(kernel='rbf', C=1E6)\n",
    "\n",
    "# Fit\n",
    "svmk.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:18.791167Z",
     "start_time": "2020-07-30T06:31:18.616731Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "plot_svc_decision_function(svmk)\n",
    "plt.scatter(svmk.support_vectors_[:, 0], svmk.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick enabled finding a nonlinear decision boundary. And we're able to leverage fast linear methods to find non-linear solutions just as quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Tuning\n",
    "\n",
    "What happens when we have ill-defined boundaries between our clusters? How do we tune SVM to improve our results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:20.068406Z",
     "start_time": "2020-07-30T06:31:19.938188Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=70, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at data that, while not overlapping is still very closely spaced.\n",
    "\n",
    "Remember that the margins, in general, are defined by the support vectors (points that define the the edge of the margin) and no points can lie inside the margins. The hyperparameter $C$, is used to define the \"hardness\" of the margins. That is, larger values of $C$ correspond to harder margins and no points can lie inside them. Smaller values of $C$ will allow some points to exist inside the margins.\n",
    "\n",
    "As with other methods we can use grid-search and cross-validation to find optimal values for $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:20.987103Z",
     "start_time": "2020-07-30T06:31:20.693021Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, .5]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:21.217099Z",
     "start_time": "2020-07-30T06:31:21.087545Z"
    }
   },
   "outputs": [],
   "source": [
    "faces = fetch_lfw_people(min_faces_per_person=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:21.700756Z",
     "start_time": "2020-07-30T06:31:21.694560Z"
    }
   },
   "outputs": [],
   "source": [
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:31:22.635518Z",
     "start_time": "2020-07-30T06:31:22.185914Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:36:47.495050Z",
     "start_time": "2020-07-30T06:36:47.486465Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pca = PCA(n_components=150, whiten=True, svd_solver='randomized', random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "pipeline = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:36:48.589157Z",
     "start_time": "2020-07-30T06:36:48.572393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:38:44.126986Z",
     "start_time": "2020-07-30T06:38:13.715121Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(pipeline, param_grid)\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:38:52.928315Z",
     "start_time": "2020-07-30T06:38:52.820015Z"
    }
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:03.232973Z",
     "start_time": "2020-07-30T06:39:02.459138Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='green' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:07.748591Z",
     "start_time": "2020-07-30T06:39:07.717101Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that precision penalizes false positives and recall false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:11.168631Z",
     "start_time": "2020-07-30T06:39:10.599238Z"
    }
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier\n",
    "\n",
    "\n",
    "- [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "**Background** The goal of a support vector machine is to find  the optimal separating hyperplane which maximizes the margin of the training data. \n",
    "\n",
    "**Pros**:\n",
    "    - Powerful Model\n",
    "    - Popularized modern machine learning due to it's extreme power (dethroned by Deep Learning)\n",
    "    - Robust to outliers\n",
    "    - Uses the kernel trick \n",
    "      \n",
    "**Cons**:\n",
    "    - Many possible settings\n",
    "    - Slow to train\n",
    "    - Scale matters\n",
    "    - Can be a black box (it's hard to understand how or why it makes predictions)\n",
    "    - Does not provide predicted probabilities\n",
    "    \n",
    "**What do we know?**\n",
    "\n",
    "- It needs training data so it's a supervised model\n",
    "- We're using it for classification\n",
    "\n",
    "**What do we not know**\n",
    "- How does it make predictions?\n",
    "- What is a hyperplane?\n",
    "\n",
    "Those questions are inherently linked. A **hyperplane** is the seperation of space between our classes. If we break it down further we can better understand it.\n",
    "\n",
    "- in one dimension, an hyperplane is called a point\n",
    "- in two dimensions, it is a line\n",
    "- in three dimensions, it is a plane\n",
    "- in more dimensions you can call it an hyperplane\n",
    "\n",
    "As we start to understand that concept it begs another question. I can draw alot of lines so which is the right one? That's the goal of the SVM - **Finding the optimal hyperplane**. Finding this optimal hyperplane is dependent on a few particular vectors that support its placement - or support vectors.\n",
    "\n",
    "Support Vectors are the data points closest to the hyperplane or decision line. These are the data points that are the **most difficult** to classify. Given their proximity to the hyperplane they have direct bearing on its optimum location. Essentially Support vectors would change the elements of the training set if moved or removed and are critical elements of the training set.\n",
    "\n",
    "![image.png](../assets/SupportVector.png)\n",
    "\n",
    "\n",
    "**Now to optimize our hyperlane!**\n",
    "\n",
    "Step 1 - Seperate the plane as far as you can from data\n",
    "![Optimal Hyperplane](../assets/optimal-hyperplane.png)\n",
    "\n",
    "Step 2 - Find the hyperplane with the largest margin\n",
    "\n",
    "For any hyperplane we can compute the margin.\n",
    " - Find the distance between the hyperplane and the closest data point. \n",
    " - Take that distance and double it\n",
    " \n",
    "Now you have the **margin**. Basically the margin is an area where you will not find any data points. (Note: this can cause some problems when data is noisy)\n",
    "\n",
    "![Margin](../assets/margin.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Kernel Trick: For when our data isn't already linearly separable\n",
    "\n",
    "The below picture shows the true magic behind a Support Vector Machine. The objects on the left are mapped as we'd originally find them. A full seperation would require a curve and thus more complexity than drawing a line. In a support vector machine we rearrange using a set of mathmatical functions known as **kernels**. An intuitive way to think about kernels is a similarity function. Given two objects the kernel outputs some similarity score. The simpliest example is the linear kernel or dot-product. Given two vectors, the similarity is the lenght of hte projections of one vector to another. Given a data point to classify, the decision function makes use of the kernel by comparing that data point to a number of support vectors weighted by the learned parameters. The support vectors are in the domain of that data point and along the learned parameters are found by the learning algorithm. \n",
    "\n",
    "![Kernels](../assets/Input_Feature.gif)\n",
    "\n",
    "### Sometimes it's helpful to see this in 3d\n",
    "\n",
    "While it's hard to draw a line to divide the classes in the picture on the left, it's easy on the right\n",
    "\n",
    "![Kernels-2d-to-3d](../assets/2d-to-3d.png)\n",
    "\n",
    "### Another example\n",
    "\n",
    "<img src=\"../assets/2d-to-3d-projection.jpeg\" width=\"600\">\n",
    "\n",
    "#### Parameters\n",
    "\n",
    ">class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "\n",
    "Today we're going to adjust C. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly\n",
    "by giving the model freedom to select more samples as support vectors. \n",
    "\n",
    "![BigC_LittleC](../assets/BigC_LittleC.png)\n",
    "\n",
    "As we regularize, or penalize C, we can visualize the impact on our predictions\n",
    "![Regularize](../assets/c_regulation.png)\n",
    "\n",
    "Note: \n",
    "* Here's a good source on [understanding the math](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/) or [The Idiot's Guide to SVM](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)\n",
    "* Learn more about [Kernels](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:37.708223Z",
     "start_time": "2020-07-30T06:39:37.697402Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Structure:\n",
    "\n",
    "In this lesson, you will build two types of classifiers\n",
    "\n",
    "1. A [Logistic Regression Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. A [Suport Vector Machine Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "\n",
    "We will train these models on a dataset of tumors, aiming to classify a given tumor as benign or malignant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "---\n",
    "**1. EDA**\n",
    "    - Analyze the dataset, mostly through visualizations.\n",
    "**2. Data Preparation**\n",
    "    - Handle non-numeric features.\n",
    "    - Handle missing values.\n",
    "    - Compute the baseline accuracy.\n",
    "**3. Model Training**\n",
    "    - Performing a train-test split on our data\n",
    "    - Fitting the models of your choice (today: LogisticRegression and SVC) on the training data.\n",
    "    - Scoring our trained models on the test data.\n",
    "    \n",
    "**4. Model Evaluation**\n",
    "    - Gaining an understanding of how our model makes predictions.\n",
    "    - Tuning our model to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.biosytechworld.com/media/3188/123.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:42.853294Z",
     "start_time": "2020-07-30T06:39:42.761152Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cancer_uci.csv', index_col='Unnamed: 0')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:47.440242Z",
     "start_time": "2020-07-30T06:39:47.431067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of rows and columns of our dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:47.835995Z",
     "start_time": "2020-07-30T06:39:47.821646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the data types and verify we have no missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:48.297755Z",
     "start_time": "2020-07-30T06:39:48.287918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:48.723840Z",
     "start_time": "2020-07-30T06:39:48.715593Z"
    }
   },
   "outputs": [],
   "source": [
    "# For our non-numeric columns, explore how many (and what) categories we have\n",
    "df['Bare_Nuclei'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:49.220838Z",
     "start_time": "2020-07-30T06:39:49.200066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking for pairwise correlations between the features (Heatmap)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:50.464093Z",
     "start_time": "2020-07-30T06:39:49.772211Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: As a learning experience, run sns.pairplot on this data\n",
    "# Also try: sns.pairplot(df, hue='Class', plot_kws={'alpha':0.1})\n",
    "sns.heatmap(df.corr(),annot=True,linewidths=0.01, cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:51.762976Z",
     "start_time": "2020-07-30T06:39:51.312642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Groupby is also a great EDA tool\n",
    "\n",
    "df.groupby('Class').mean().plot(kind='bar', figsize=(20,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:52.446942Z",
     "start_time": "2020-07-30T06:39:52.440711Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop columns you will not use\n",
    "data = df.drop(['Sample_code_number'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:52.986045Z",
     "start_time": "2020-07-30T06:39:52.981315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle 'bad data'\n",
    "data['Bare_Nuclei'] = data['Bare_Nuclei'].apply(lambda x: np.nan if x =='?' else float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing a python library\n",
    "\n",
    "To install `missingno` run `!pip install missingno` in a cell. You only ever need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:58.544379Z",
     "start_time": "2020-07-30T06:39:57.385052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discover bad data:\n",
    "# Option 1: data.isnull().sum()\n",
    "# Option 2: data.isnull().mean()\n",
    "\n",
    "msgn.matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:39:59.631413Z",
     "start_time": "2020-07-30T06:39:59.623949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "\n",
    "clean_data = data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:01.277151Z",
     "start_time": "2020-07-30T06:40:00.400646Z"
    }
   },
   "outputs": [],
   "source": [
    "msgn.matrix(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:04.633690Z",
     "start_time": "2020-07-30T06:40:04.626553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features from target\n",
    "y = clean_data['Class']\n",
    "X = clean_data.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:05.689348Z",
     "start_time": "2020-07-30T06:40:05.678065Z"
    }
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:06.160658Z",
     "start_time": "2020-07-30T06:40:06.151686Z"
    }
   },
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:07.313237Z",
     "start_time": "2020-07-30T06:40:07.309474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a logistic regression model\n",
    "\n",
    "lr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:08.133498Z",
     "start_time": "2020-07-30T06:40:08.130047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a SVM model\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:09.142255Z",
     "start_time": "2020-07-30T06:40:09.134270Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up an 80-20 Train-test split with random_state=1987\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:16.859208Z",
     "start_time": "2020-07-30T06:40:16.829816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit and score the logistic regression\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "lr.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:21.477936Z",
     "start_time": "2020-07-30T06:40:21.464492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit and score the SVM model\n",
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Evaluation\n",
    "\n",
    "We will discuss the evaluation of classification models in detail either at the end of this class or during the next lesson. For now, our main concern is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:26.522643Z",
     "start_time": "2020-07-30T06:40:26.488399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reminding ourselves of the baseline we are trying to beat\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:27.347497Z",
     "start_time": "2020-07-30T06:40:27.338876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking at the model accuracy\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is very accurate, but is it confident in its predictions? Could we trust it to automate the diagnosis of some patients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:31.559139Z",
     "start_time": "2020-07-30T06:40:31.554248Z"
    }
   },
   "outputs": [],
   "source": [
    "y_proba = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:32.493641Z",
     "start_time": "2020-07-30T06:40:32.194825Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_proba[:,1], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:33.567547Z",
     "start_time": "2020-07-30T06:40:33.095356Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_proba[:,1], bins=100, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:33.660617Z",
     "start_time": "2020-07-30T06:40:33.657339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ideally we'd also spend some time looking at the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:35.093903Z",
     "start_time": "2020-07-30T06:40:35.043927Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Reading the data\n",
    "df = pd.read_csv('./data/cancer_uci.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# Dropping an unusable column\n",
    "data = df.drop(['Sample_code_number'], axis=1)\n",
    "\n",
    "# Identifying missing numbers\n",
    "data['Bare_Nuclei'] = data['Bare_Nuclei'].apply(lambda x: np.nan if x =='?' else float(x))\n",
    "\n",
    "# Dropping the few rows with missing numbers\n",
    "clean_data = data.dropna(how='any')\n",
    "\n",
    "# Separating the target from the features\n",
    "y = clean_data['Class']\n",
    "X = clean_data.drop('Class', axis=1)\n",
    "\n",
    "# Computing the baseline\n",
    "print('Baseline:', y.value_counts(normalize=True)[0].round(3))\n",
    "\n",
    "# Instantiating a Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Instantiating a Support Vector Machine Model\n",
    "svc = SVC()\n",
    "\n",
    "# Fitting the models\n",
    "lr.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Scoring the models:\n",
    "print('Logistic Regression:', lr.score(X_test, y_test).round(3))\n",
    "print('Support Vector Machine:', svc.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS WORK 1\n",
    "\n",
    "Try using other `kernels` and `C` values in the SVM model to obtain a higher accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS WORK 2\n",
    "\n",
    "\n",
    "![](https://www.researchgate.net/profile/Juan_Banda/publication/256418526/figure/fig1/AS:297921313558528@1448041384565/Confusion-matrix-example.png)\n",
    "\n",
    "# Confusion Matrix\n",
    "This is a good (if difficult) way to look at your model and where it makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:40.340114Z",
     "start_time": "2020-07-30T06:40:40.273401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here I'm training a deliberately good and bad model\n",
    "\n",
    "svc_awesome = SVC(kernel='rbf')\n",
    "svc_good = SVC(kernel='rbf', C=0.01)\n",
    "svc_bad = SVC(kernel='sigmoid')\n",
    "\n",
    "svc_awesome.fit(X_train, y_train)\n",
    "svc_good.fit(X_train, y_train)\n",
    "svc_bad.fit(X_train, y_train)\n",
    "\n",
    "print(svc_awesome.score(X_test, y_test))\n",
    "print(svc_good.score(X_test, y_test))\n",
    "print(svc_bad.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at confusion matrices again\n",
    "\n",
    "A confusion matrix allows you to visualize the performance of an algorithm on a test set of data vs known true values. There are a few methods \n",
    "\n",
    "| Metric  | Description  | Output  |\n",
    "|-----|-----|-----|\n",
    "|[metrics.confusion_matrix(y_true, y_pred)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)| Compute confusion matrix to evaluate the accuracy of a classification | Grid |\n",
    "|[metrics.multilabel_confusion_matrix(y_true, …)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html#sklearn.metrics.multilabel_confusion_matrix)| Compute a confusion matrix for each class or sample | Grid |\n",
    "|[metrics.plot_confusion_matrix(estimator, X, …)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix)| Plot Confusion Matrix | Plot |\n",
    "|[metrics.ConfusionMatrixDisplay(...)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)| Confusion Matrix visualization| Plot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:40:55.666809Z",
     "start_time": "2020-07-30T06:40:55.656541Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix(y_test, svc_awesome.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Juan_Banda/publication/256418526/figure/fig1/AS:297921313558528@1448041384565/Confusion-matrix-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:41:00.786646Z",
     "start_time": "2020-07-30T06:41:00.782658Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix(y_true, X, model):\n",
    "    classes = model.classes_\n",
    "    confusion_matrix_values = confusion_matrix(y_true, model.predict(X), labels=classes)\n",
    "    data_frame = pd.DataFrame(confusion_matrix_values, columns=classes, index=classes)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:15:35.990703Z",
     "start_time": "2020-07-30T14:15:35.715088Z"
    }
   },
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_test,X_test, svc_awesome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or... my way\n",
    "\n",
    "My motto is \n",
    "\n",
    ">**Data Scientists are curious but inherently lazy.**\n",
    "\n",
    "If you find confusion matrices... well confusing then there's probably a better way. The number of TP/FP/TN/FN are important, but the ratios and metrics derived from them are more important. Let's resort to just using those\n",
    "\n",
    "[Documentation in Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T06:41:07.373978Z",
     "start_time": "2020-07-30T06:41:07.360326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the classification_report package in sklearn.metrics gives the results we want.\n",
    "\n",
    "print(classification_report(y_test, svc_awesome.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember precision is penalized by false positives and recall by false negatives:\n",
    "\n",
    "$$\\text{Precision:    } \\frac{TP}{TP+FP}$$\n",
    "\n",
    "$$\\text{Recall:    } \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$\\text{F1:    } \\frac{\\text{Precision} * \\text{Recall} }{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "\n",
    "Support is simply the number of samples in that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:16:09.114120Z",
     "start_time": "2020-07-30T14:16:09.066742Z"
    }
   },
   "outputs": [],
   "source": [
    "# What is the confusion matrix for the good model\n",
    "make_confusion_matrix(y_test,X_test, svc_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:16:17.821185Z",
     "start_time": "2020-07-30T14:16:17.795044Z"
    }
   },
   "outputs": [],
   "source": [
    "# What is the confusion matrix for the bad model\n",
    "make_confusion_matrix(y_test,X_test, svc_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for thought\n",
    "\n",
    "In the case of a model that predicts if a tumor is benign or malignant, which of the following should you pay the most attention to?\n",
    "\n",
    "1. True Positive Rate\n",
    "2. True Negative Rate\n",
    "3. False Positive Rate\n",
    "4. False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
